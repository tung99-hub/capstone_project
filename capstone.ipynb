{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc17119",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b40e7",
   "metadata": {},
   "source": [
    "## Import all necessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5796a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, \\\n",
    "                            date_format, dayofweek, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, \\\n",
    "                                IntegerType, DateType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import boto3\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18516192",
   "metadata": {},
   "source": [
    "## Set environment variables (AWS credentials) so Spark can access the S3 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ffe053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('aws_credentials.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7ce54",
   "metadata": {},
   "source": [
    "## Initialize SparkSession instance, with hadoop-aws package to process S3 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35cce3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('capstone') \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.1.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Settings to allow the parsing of some timestamps\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb076619",
   "metadata": {},
   "source": [
    "## Set up paths to csv files containing the data. There is also a json file, which will be handled separately.\n",
    "\n",
    "These files can be found in the ```data``` folder of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f4d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://tung99-bucket/\"\n",
    "\n",
    "zone_data = os.path.join(input_data, \"taxi+_zone_lookup.csv\")\n",
    "temp_data = os.path.join(input_data, \"Hyperlocal_Temperature_Monitoring.csv\")\n",
    "trips_data = os.path.join(input_data, \"yellow_tripdata_2018-07.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057fb16",
   "metadata": {},
   "source": [
    "## Process each of the csv file above into a DataFrame\n",
    "\n",
    "After a brief inspection on the data files, these issues have been observed:\n",
    "- Some of the entries in the ```nta_name``` column of the taxi zone lookup table contain more than one NTA name, making it impossible to match the names with the correct NTA code. To circumvent this, the problematic rows are split into identical ones using the ```/``` delimiter.\n",
    "- Similarly, some NTA names were combined by ```-``` in the ```nta_codes.json``` file, requiring the use of splitting to separate the names.\n",
    "While these fixes cannot guarantee that all location IDs will be mapped to an NTA code, it does significantly increases the number of matches (from 108 to 238 after the fixes).\n",
    "- There is an empty row in the beginning of the trips file, but it will go away as we do the ```JOIN``` operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4082d3",
   "metadata": {},
   "source": [
    "### Taxi zone lookup file (this connects the location ID to an NTA name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51aec1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------+------------+\n",
      "|location_id|boro_name|      nta_name|service_zone|\n",
      "+-----------+---------+--------------+------------+\n",
      "|          1|      EWR|Newark Airport|         EWR|\n",
      "|          2|   Queens|   Jamaica Bay|   Boro Zone|\n",
      "|          3|    Bronx|      Allerton|   Boro Zone|\n",
      "|          3|    Bronx|Pelham Gardens|   Boro Zone|\n",
      "|          4|Manhattan| Alphabet City| Yellow Zone|\n",
      "+-----------+---------+--------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-populate schema\n",
    "zoneSchema = StructType([\n",
    "    StructField(\"location_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"boro_name\", StringType(), nullable=False),\n",
    "    StructField(\"nta_name\", StringType(), nullable=False),\n",
    "    StructField(\"service_zone\", StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "zone_df = spark.read.csv(zone_data, header=True, schema=zoneSchema)\n",
    "# Splitting happens here, see that the 3rd and 4th rows are almost the same.\n",
    "zone_df = zone_df.withColumn('nta_name', F.explode(F.split('nta_name', '/')))\n",
    "\n",
    "zone_df.createOrReplaceTempView(\"zones\")\n",
    "zone_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbbb3fb",
   "metadata": {},
   "source": [
    "### Temperature information file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41319c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+----+-----------+------------+----+------------+---------+--------+-----+---+\n",
      "|sensor_id|   air_temp|               date|hour|   latitude|   longitude|year|install_type|boro_name|nta_code|month|day|\n",
      "+---------+-----------+-------------------+----+-----------+------------+----+------------+---------+--------+-----+---+\n",
      "| Bk-BR_01|     71.189|2018-06-15 00:00:00|   1|40.66620508|-73.91691035|2018| Street Tree| Brooklyn|    BK81|    6| 15|\n",
      "| Bk-BR_01|70.24333333|2018-06-15 00:00:00|   2|40.66620508|-73.91691035|2018| Street Tree| Brooklyn|    BK81|    6| 15|\n",
      "| Bk-BR_01|69.39266667|2018-06-15 00:00:00|   3|40.66620508|-73.91691035|2018| Street Tree| Brooklyn|    BK81|    6| 15|\n",
      "| Bk-BR_01|68.26316667|2018-06-15 00:00:00|   4|40.66620508|-73.91691035|2018| Street Tree| Brooklyn|    BK81|    6| 15|\n",
      "| Bk-BR_01|     67.114|2018-06-15 00:00:00|   5|40.66620508|-73.91691035|2018| Street Tree| Brooklyn|    BK81|    6| 15|\n",
      "+---------+-----------+-------------------+----+-----------+------------+----+------------+---------+--------+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-populate the schema\n",
    "tempSchema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), nullable=False),\n",
    "    StructField(\"air_temp\", DoubleType(), nullable=False),\n",
    "    StructField(\"date\", StringType(), nullable=False),\n",
    "    StructField(\"hour\", IntegerType(), nullable=False),\n",
    "    StructField(\"latitude\", DoubleType(), nullable=False),\n",
    "    StructField(\"longitude\", DoubleType(), nullable=False),\n",
    "    StructField(\"year\", IntegerType(), nullable=False),\n",
    "    StructField(\"install_type\", StringType(), nullable=False),\n",
    "    StructField(\"boro_name\", StringType(), nullable=False),\n",
    "    StructField(\"nta_code\", StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "temp_df = spark.read.csv(temp_data, header=True, schema=tempSchema)\n",
    "# Convert original dates (in string) to timestamp format\n",
    "temp_df = temp_df.withColumn(\"date\", F.to_timestamp(\"date\", \"M/dd/yyyy\"))\n",
    "# Additional columns added to make comparisons with other DataFrames easier\n",
    "temp_df = temp_df.withColumn(\"month\", F.month(\"date\"))\n",
    "temp_df = temp_df.withColumn(\"day\", F.dayofmonth(\"date\"))\n",
    "\n",
    "temp_df.createOrReplaceTempView(\"temperatures\")\n",
    "temp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832195f",
   "metadata": {},
   "source": [
    "### Taxi trips file. For simplicity reason, only trips happening in July 2018 were included, so a lot of assumptions have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8b8f78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+-----------+------------------+--------------+--------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----+------+------+-------+-------+\n",
      "|vendor_id|            PU_date|            DO_date|passenger_count|trip_distance|ratecode_id|store_and_fwd_flag|PU_location_id|DO_location_id|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|month|PU_day|DO_day|PU_hour|DO_hour|\n",
      "+---------+-------------------+-------------------+---------------+-------------+-----------+------------------+--------------+--------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----+------+------+-------+-------+\n",
      "|     null|               null|               null|           null|         null|       null|              null|          null|          null|        null|       null| null|   null|      null|        null|                 null|        null| null|  null|  null|   null|   null|\n",
      "|        1|2018-07-01 00:28:00|2018-07-01 00:28:00|              1|          5.3|          1|                 N|           145|           145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|    7|     1|     1|      0|      0|\n",
      "|        1|2018-07-01 00:29:00|2018-07-01 00:30:00|              1|          5.3|          1|                 N|           145|           145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|    7|     1|     1|      0|      0|\n",
      "|        1|2018-07-01 00:04:00|2018-07-01 00:08:00|              2|          0.7|          1|                 N|           211|           144|           1|        5.0|  0.5|    0.5|      1.25|         0.0|                  0.3|        7.55|    7|     1|     1|      0|      0|\n",
      "|        1|2018-07-01 00:14:00|2018-07-01 00:36:00|              1|          4.8|          1|                 N|           144|           142|           1|       18.0|  0.5|    0.5|       1.0|         0.0|                  0.3|        20.3|    7|     1|     1|      0|      0|\n",
      "+---------+-------------------+-------------------+---------------+-------------+-----------+------------------+--------------+--------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+-----+------+------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-populate the schema\n",
    "tripsSchema = StructType([\n",
    "    StructField(\"vendor_id\", IntegerType(), nullable=True),\n",
    "    StructField(\"PU_date\", StringType(), nullable=True),\n",
    "    StructField(\"DO_date\", StringType(), nullable=True),\n",
    "    StructField(\"passenger_count\", IntegerType(), nullable=True),\n",
    "    StructField(\"trip_distance\", DoubleType(), nullable=True),\n",
    "    StructField(\"ratecode_id\", IntegerType(), nullable=True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), nullable=True),\n",
    "    StructField(\"PU_location_id\", IntegerType(), nullable=True),\n",
    "    StructField(\"DO_location_id\", IntegerType(), nullable=True),\n",
    "    StructField(\"payment_type\", IntegerType(), nullable=True),\n",
    "    StructField(\"fare_amount\", DoubleType(), nullable=True),\n",
    "    StructField(\"extra\", DoubleType(), nullable=True),\n",
    "    StructField(\"mta_tax\", DoubleType(), nullable=True),\n",
    "    StructField(\"tip_amount\", DoubleType(), nullable=True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), nullable=True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), nullable=True),\n",
    "    StructField(\"total_amount\", DoubleType(), nullable=True)\n",
    "])\n",
    "\n",
    "trips_df = spark.read.csv(trips_data, header=True, schema=tripsSchema)\n",
    "# Convert strings to timestamps\n",
    "trips_df = trips_df.withColumn(\"PU_date\", F.to_timestamp(\"PU_date\", \"M/d/yyyy H:mm\"))\n",
    "trips_df = trips_df.withColumn(\"DO_date\", F.to_timestamp(\"DO_date\", \"M/d/yyyy H:mm\"))\n",
    "# Add additional columns\n",
    "trips_df = trips_df.withColumn(\"month\", F.month(\"PU_date\"))\n",
    "trips_df = trips_df.withColumn(\"PU_day\", F.dayofmonth(\"PU_date\"))\n",
    "trips_df = trips_df.withColumn(\"DO_day\", F.dayofmonth(\"DO_date\"))\n",
    "trips_df = trips_df.withColumn(\"PU_hour\", F.hour(\"PU_date\"))\n",
    "trips_df = trips_df.withColumn(\"DO_hour\", F.hour(\"DO_date\"))\n",
    "\n",
    "trips_df.createOrReplaceTempView(\"trips\")\n",
    "trips_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04f8b9",
   "metadata": {},
   "source": [
    "### Process the nta_codes.json file. The NTA names that contain ```-``` are split into separate rows, but they have the same NTA code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089ee6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+\n",
      "|boro_name|nta_code|     nta_name|\n",
      "+---------+--------+-------------+\n",
      "| Brooklyn|    BK88| Borough Park|\n",
      "|   Queens|    QN51|  Murray Hill|\n",
      "|   Queens|    QN27|East Elmhurst|\n",
      "| Brooklyn|    BK23|West Brighton|\n",
      "|   Queens|    QN41|Fresh Meadows|\n",
      "+---------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "content_object = s3.Object('tung99-bucket', 'nta_codes.json')\n",
    "file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "json_content = json.loads(file_content)\n",
    "nta_data = json_content[\"data\"]\n",
    "nta_list = []\n",
    "\n",
    "for nta in nta_data:\n",
    "    for sub_nta in nta[13].split('-'):\n",
    "        nta_list.append((nta[11], nta[12], sub_nta))\n",
    "    \n",
    "rdd = spark.sparkContext.parallelize(nta_list)\n",
    "ntaSchema = StructType([\n",
    "    StructField(\"boro_name\", StringType(), nullable=False),\n",
    "    StructField(\"nta_code\", StringType(), nullable=False),\n",
    "    StructField(\"nta_name\", StringType(), nullable=False)\n",
    "])\n",
    "nta_df = spark.createDataFrame(rdd, schema=ntaSchema)\n",
    "\n",
    "nta_df.createOrReplaceTempView(\"ntas\")\n",
    "nta_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de143b02",
   "metadata": {},
   "source": [
    "## Convert information gathered from the above DataFrames into specific tables.\n",
    "Each code cell shows the processing of one table (with 4 in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b84f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-------+----+-------+\n",
      "|year|month|day|weekday|hour|time_id|\n",
      "+----+-----+---+-------+----+-------+\n",
      "|2018|    7|  4|      4|   2|      0|\n",
      "|2018|    7|  4|      4|  17|      1|\n",
      "|2018|    7|  5|      5|  11|      2|\n",
      "|2018|    7|  6|      6|  12|      3|\n",
      "|2018|    7| 25|      4|   0|      4|\n",
      "+----+-----+---+-------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table = spark.sql('''\n",
    "    SELECT year(date) AS year, month(date) AS month, dayofmonth(date) AS day, dayofweek(date) AS weekday, hour\n",
    "    FROM temperatures\n",
    "    WHERE year(date)=2018 AND month(date)=7\n",
    "''').distinct().withColumn('time_id', monotonically_increasing_id())\n",
    "\n",
    "time_table.createOrReplaceTempView(\"times\")\n",
    "time_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "429f347e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-------------+---------------+-------------+--------------+--------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|vendor_id|  PU_date_id|   DO_date_id|passenger_count|trip_distance|PU_location_id|DO_location_id|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+---------+------------+-------------+---------------+-------------+--------------+--------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|        2|283467841536|1125281431552|              3|         1.28|           246|           234|           1|        7.5|  0.5|    0.5|      1.76|         0.0|                  0.3|       10.56|\n",
      "|        2| 25769803778|1125281431552|              1|         2.01|           164|            79|           2|        8.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         9.3|\n",
      "|        2| 25769803778|1125281431552|              1|         2.96|            79|           230|           2|       12.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        13.3|\n",
      "|        2| 25769803778|1125281431552|              1|         1.46|            68|           137|           1|        7.5|  0.5|    0.5|      1.76|         0.0|                  0.3|       10.56|\n",
      "|        2|214748364801| 214748364801|              2|        17.96|           132|           107|           1|       52.0|  0.0|    0.5|     10.56|         0.0|                  0.3|       63.36|\n",
      "+---------+------------+-------------+---------------+-------------+--------------+--------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_table = spark.sql('''\n",
    "    SELECT vendor_id, times.time_id AS PU_date_id, DO_day, t.month, \n",
    "            DO_hour, passenger_count, trip_distance, PU_location_id, \n",
    "            DO_location_id, payment_type, fare_amount, extra, mta_tax,\n",
    "            tip_amount, tolls_amount, improvement_surcharge, total_amount\n",
    "    FROM trips t\n",
    "    JOIN times ON t.PU_day=times.day AND t.month=times.month AND t.PU_hour=times.hour\n",
    "''')\n",
    "\n",
    "trips_table.createOrReplaceTempView(\"trips\")\n",
    "trips_table = spark.sql('''\n",
    "    SELECT vendor_id, PU_date_id, times.time_id AS DO_date_id, passenger_count, \n",
    "            trip_distance, PU_location_id, DO_location_id, payment_type, \n",
    "            fare_amount, extra, mta_tax, tip_amount, tolls_amount, \n",
    "            improvement_surcharge, total_amount\n",
    "    FROM trips t\n",
    "    JOIN times ON t.DO_day=times.day AND t.month=times.month AND t.DO_hour=times.hour\n",
    "''')\n",
    "\n",
    "trips_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ab5a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+-------------+------------+\n",
      "|location_id|    boro_name|nta_code|     nta_name|service_zone|\n",
      "+-----------+-------------+--------+-------------+------------+\n",
      "|         26|     Brooklyn|    BK88| Borough Park|   Boro Zone|\n",
      "|        170|    Manhattan|    QN51|  Murray Hill| Yellow Zone|\n",
      "|         70|       Queens|    QN27|East Elmhurst|   Boro Zone|\n",
      "|        245|Staten Island|    BK23|West Brighton|   Boro Zone|\n",
      "|         98|       Queens|    QN41|Fresh Meadows|   Boro Zone|\n",
      "+-----------+-------------+--------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loc_table = spark.sql('''\n",
    "    SELECT location_id, z.boro_name, nta_code, z.nta_name, service_zone\n",
    "    FROM zones z\n",
    "    JOIN ntas ON z.nta_name=ntas.nta_name\n",
    "''')\n",
    "\n",
    "loc_table.createOrReplaceTempView(\"locations\")\n",
    "loc_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d0283b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+------------+\n",
      "|      time_id|location_id|   air_temp|install_type|\n",
      "+-------------+-----------+-----------+------------+\n",
      "|1022202216448|         75|73.18433333| Street Tree|\n",
      "|1022202216448|         75|72.95483333| Street Tree|\n",
      "|1022202216448|         75|71.95583333| Street Tree|\n",
      "|1022202216448|         75|73.24216667| Street Tree|\n",
      "|1022202216448|         75|    72.7315| Street Tree|\n",
      "+-------------+-----------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temps_table = spark.sql('''\n",
    "    SELECT times.time_id, l.location_id, t.air_temp, t.install_type\n",
    "    FROM temperatures t\n",
    "    JOIN times ON times.month=t.month AND times.day=t.day AND times.hour=t.hour\n",
    "    JOIN locations l ON l.nta_code=t.nta_code\n",
    "''')\n",
    "\n",
    "temps_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc751005",
   "metadata": {},
   "source": [
    "## Data quality checks happen after all tables have been made:\n",
    "- Since the data only comes from the month of July, there should be exactly 744 entries corresponding to 31 days of July (and each day contains 24 hours).\n",
    "- All the tables should not contain any ```NULL```s, as there was only one ```NULL``` row in the beginning, which should be eliminated after the ```JOIN``` on the ```trips``` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecfe3a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null values found.\n"
     ]
    }
   ],
   "source": [
    "if time_table.count() != 31*24:\n",
    "    raise ValueError('Some hours are missing!')\n",
    "\n",
    "table_list = [trips_table, time_table, temps_table, loc_table]\n",
    "\n",
    "for table in table_list:\n",
    "    # Check for NULL and NaN in every column of each table\n",
    "    null_count = table.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) \\\n",
    "                               for c in table.columns]).rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x + y).collect()[0][1]\n",
    "    if null_count != 0:\n",
    "        raise ValueError('Null values(s) found!')\n",
    "print('No null values found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315072f6",
   "metadata": {},
   "source": [
    "## Finally, the resulting tables are written back to an S3 bucket in the form of parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6dae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bucket = \"s3a://tung99-bucket/\"\n",
    "\n",
    "trips_output = os.path.join(output_bucket, \"trips\")\n",
    "temperatures_output = os.path.join(output_bucket, \"temperatures\")\n",
    "locations_output = os.path.join(output_bucket, \"locations\")\n",
    "times_output = os.path.join(output_bucket, \"times\")\n",
    "\n",
    "trips_table.write.parquet(trips_output, mode='overwrite', partitionBy=['PU_location_id', 'DO_location_id'])\n",
    "temps_table.write.parquet(temperatures_output, mode='overwrite', partitionBy='location_id')\n",
    "loc_table.write.parquet(locations_output, mode='overwrite', partitionBy='service_zone')\n",
    "time_table.write.parquet(times_output, mode='overwrite', partitionBy='weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85368529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
